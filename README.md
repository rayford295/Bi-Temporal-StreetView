# ğŸ›°ï¸ BiTemporal-StreetView-Damage

**Hyperlocal disaster damage assessment using bi-temporal street-view imagery and pre-trained vision models**

<p align="center">
  <img src="https://github.com/rayford295/BiTemporal-StreetView-Damage/blob/main/images/0204-06.png" alt="Study Area Map" width="600"/>
</p>

---

## ğŸ“˜ Overview

This repository presents a **bi-temporal street-view image analysis framework** for **hyperlocal disaster damage assessment**.  
By integrating **pre- and post-disaster imagery** through **pre-trained vision and visionâ€“language models**, this approach improves both classification accuracy and interpretability of damage detection.

### ğŸ” Key Contributions

- âœ… **Dual-channel architecture** for preâ€“ and post-disaster fusion.  
- ğŸ“¸ **2,249 labeled street-view image pairs**, annotated with detailed impact levels.  
- ğŸ“ˆ **Performance Gain**: Accuracy increased from 66.14% (post-only) â†’ **77.11% (bi-temporal)**.  
- ğŸ”¥ **Grad-CAM visualization** demonstrates improved attention focus using pre-disaster inputs.  
- ğŸ™ï¸ Supports **fine-grained and rapid damage mapping** for climate-resilient urban planning.

---

## ğŸ§© Methodology

<p align="center">
  <img src="https://github.com/rayford295/Bi-Temporal-StreetView/blob/main/images/dual_channel.drawio%20(2).png" alt="Dual-Channel Architecture" width="700"/>
</p>

<p align="center"><i>Figure 1: Dual-channel architecture for bi-temporal disaster damage assessment.</i></p>

### Model Pipeline
1. **Pre-processing:** Normalize paired street-view images (pre-/post-disaster).  
2. **Feature Extraction:** Use pre-trained Swin Transformer & ConvNeXt backbones.  
3. **Dual-Channel Fusion:** Fuse embeddings via a feature-fusion head for comparative reasoning.  
4. **Classification:** Predict severity levels (mild, moderate, severe).  
5. **Visualization:** Apply Grad-CAM to interpret key spatial attention areas.

---

## ğŸŒ Study Area
<p align="center">
  <img src="https://github.com/rayford295/Bi-Temporal-StreetView/blob/main/images/study_area_disaster%20damage_made.png" alt="Study Area Map" width="700"/>
</p>

The study focuses on **Horseshoe Beach, Florida**, which was severely impacted by **Hurricane Milton (2024)**.  
Bi-temporal street-view imagery was collected to model the extent and types of disaster damage across different locations.

<p align="center">
  <img src="https://github.com/rayford295/Bi-Temporal-StreetView/blob/main/images/heatmap%20all.drawio.png" alt="Damage Distribution Heatmap" width="700"/>
</p>

<p align="center"><i>Figure 2: Heatmap visualization of disaster severity distribution across Horseshoe Beach, Florida.</i></p>

---

## ğŸ“‚ Dataset

You can access the **bi-temporal street-view disaster dataset** via the DOI below:

> ğŸ“ **Yang, Yifan (2025)**.  
> *Perceiving Multidimensional Disaster Damages from Streetâ€“View Images Using Visualâ€“Language Models*.  
> figshare. Dataset. [https://doi.org/10.6084/m9.figshare.28801208.v2](https://doi.org/10.6084/m9.figshare.28801208.v2)

**Dataset Contents:**
- Paired pre-/post-disaster street-view images  
- Location and damage-type annotations  
- Severity labels: *Mild, Moderate, Severe*  
- Sample imagery from **Horseshoe Beach, FL** (Hurricane Milton, 2024)

---

## ğŸ§  Paper Reference

[![CEUS](https://img.shields.io/badge/Journal-CEUS-blue.svg)](https://doi.org/10.1016/j.compenvurbsys.2025.102335)
[![DOI](https://img.shields.io/badge/DOI-10.1016%2Fj.compenvurbsys.2025.102335-blue.svg)](https://doi.org/10.1016/j.compenvurbsys.2025.102335)
[![arXiv](https://img.shields.io/badge/arXiv-2504.09066-b31b1b.svg)](https://arXiv.org/abs/2504.09066)

If you use this repository, please cite **both** the *Computers, Environment and Urban Systems* article and the *arXiv* preprint.

<details>
<summary><b>ğŸ“– APA Citation (click to expand)</b></summary>

Yang, Y., Zou, L., Zhou, B., Li, D., Lin, B., Abedin, J., & Yang, M. (2025). *Hyperlocal disaster damage assessment using bi-temporal street-view imagery and pre-trained vision models*. *Computers, Environment and Urban Systems, 121*, 102335. https://doi.org/10.1016/j.compenvurbsys.2025.102335

</details>

<details>
<summary><b>ğŸ§¾ BibTeX (click to expand)</b></summary>

```bibtex
@article{YANG2025102335,
title = {Hyperlocal disaster damage assessment using bi-temporal street-view imagery and pre-trained vision models},
journal = {Computers, Environment and Urban Systems},
volume = {121},
pages = {102335},
year = {2025},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2025.102335},
url = {https://www.sciencedirect.com/science/article/pii/S0198971525000882},
author = {Yifan Yang and Lei Zou and Bing Zhou and Daoyang Li and Binbin Lin and Joynal Abedin and Mingzheng Yang},
keywords = {Disaster resilience, Street-view imagery, Dual-channel neural network, Pre-trained vision model, Damage estimation}
}


## ğŸ—‚ Repository Structure

```bash
BiTemporal-StreetView-Damage/
â”‚
â”œâ”€â”€ codes/                          # Model training and evaluation scripts
â”œâ”€â”€ images/                         # Project figures
â”‚   â”œâ”€â”€ study_area_disaster_damage_made.png
â”‚   â”œâ”€â”€ architect1.drawio (1).png
â”‚   â”œâ”€â”€ design experiment.drawio (1).png
â”‚   â”œâ”€â”€ dual_channel.drawio (2).png
â”‚   â”œâ”€â”€ 0204-06.png
â”‚   â”œâ”€â”€ readme.txt
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore


